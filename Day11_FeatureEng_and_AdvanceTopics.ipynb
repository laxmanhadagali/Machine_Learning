{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c119e712",
   "metadata": {},
   "source": [
    "# Feature Engineering & Advanced ML — Notes, Explanations and Code\n",
    "\n",
    "_Contains: what it does, when to use, and runnable code templates (commented where needed)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e7ba64",
   "metadata": {},
   "source": [
    "## 1. Handling Missing Values\n",
    "\n",
    "**What it does:** Fills or removes missing values (NaN) so models can train properly.\n",
    "\n",
    "**When to use:** Use whenever your dataset has missing entries. Choose strategy based on data type and missingness pattern.\n",
    "\n",
    "**Common techniques:** Mean/median/mode imputation, forward/backward fill, KNN/Iterative imputer, drop rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Handling Missing Values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# sample DataFrame\n",
    "df = pd.DataFrame({'age':[25, np.nan, 37, 29],\n",
    "                   'salary':[50000, 60000, np.nan, 52000],\n",
    "                   'city':['A','B','A', None]})\n",
    "\n",
    "# 1) SimpleImputer (mean / median / most_frequent)\n",
    "mean_imp = SimpleImputer(strategy='mean')\n",
    "# df['age'] = mean_imp.fit_transform(df[['age']])\n",
    "\n",
    "# 2) KNNImputer (uses nearby rows)\n",
    "knn = KNNImputer(n_neighbors=2)\n",
    "# df[['age','salary']] = knn.fit_transform(df[['age','salary']])\n",
    "\n",
    "# 3) Forward/Backward fill\n",
    "# df.fillna(method='ffill', inplace=True)  # or 'bfill'\n",
    "\n",
    "# 4) Drop rows with too many missing values\n",
    "# df = df.dropna(subset=['age','salary'])\n",
    "\n",
    "# Note: uncomment lines above to run on your actual data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2810a23",
   "metadata": {},
   "source": [
    "## 2. Encoding Categorical Variables\n",
    "\n",
    "**What it does:** Converts string/ categorical values to numeric so ML models can use them.\n",
    "\n",
    "**When to use:** When dataset contains categorical/text columns. Choose method based on cardinality and whether order matters.\n",
    "\n",
    "**Techniques:** Label Encoding, One-Hot, Target Encoding, Frequency/Binary/Hash encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71273741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Encoding\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "df = pd.DataFrame({'city':['NY','LA','NY','SF'], 'quality':['low','high','medium','low'], 'target':[0,1,0,1]})\n",
    "\n",
    "# 1) Label Encoding (for ordinal)\n",
    "le = LabelEncoder()\n",
    "# df['quality_enc'] = le.fit_transform(df['quality'])\n",
    "\n",
    "# 2) One-Hot Encoding (for nominal) using pandas\n",
    "# df_ohe = pd.get_dummies(df, columns=['city'], drop_first=True)\n",
    "\n",
    "# 3) Target Encoding (mean target per category)\n",
    "# df['city_te'] = df.groupby('city')['target'].transform('mean')\n",
    "\n",
    "# 4) Frequency Encoding\n",
    "# freq = df['city'].value_counts().to_dict()\n",
    "# df['city_freq'] = df['city'].map(freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f2426",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling\n",
    "\n",
    "**What it does:** Brings numeric features to a similar scale which helps many algorithms converge faster and perform better.\n",
    "\n",
    "**When to use:** For distance-based models (KNN, SVM), gradient-based models (NN), or when features have very different units.\n",
    "\n",
    "**Techniques:** StandardScaler, MinMaxScaler, RobustScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c56f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'age':[20,40,60], 'income':[20000, 50000, 120000]})\n",
    "\n",
    "# Standardization (Z-score)\n",
    "scaler = StandardScaler()\n",
    "# df[['age','income']] = scaler.fit_transform(df[['age','income']])\n",
    "\n",
    "# Min-Max scaling (0-1)\n",
    "mms = MinMaxScaler()\n",
    "# df[['age','income']] = mms.fit_transform(df[['age','income']])\n",
    "\n",
    "# Robust scaling (median & IQR) - good with outliers\n",
    "rs = RobustScaler()\n",
    "# df[['age','income']] = rs.fit_transform(df[['age','income']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa6e1d5",
   "metadata": {},
   "source": [
    "## 4. Handling Outliers\n",
    "\n",
    "**What it does:** Detects and manages extreme values that can skew model learning.\n",
    "\n",
    "**When to use:** When features show extreme values or heavy tails; check with boxplots or summary stats.\n",
    "\n",
    "**Techniques:** IQR, Z-score, Winsorization, transform (log), or model-based detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5008ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Outlier detection/handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "df = pd.DataFrame({'salary':[30_000, 35_000, 40_000, 1_500_000, 45_000]})\n",
    "\n",
    "# IQR method\n",
    "Q1 = df['salary'].quantile(0.25)\n",
    "Q3 = df['salary'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "# filter valid rows\n",
    "# df_iqr = df[(df['salary'] >= Q1 - 1.5*IQR) & (df['salary'] <= Q3 + 1.5*IQR)]\n",
    "\n",
    "# Z-score method\n",
    "# z_scores = np.abs(stats.zscore(df['salary']))\n",
    "# df_z = df[z_scores < 3]\n",
    "\n",
    "# Winsorization (cap values)\n",
    "# df['salary_cap'] = df['salary'].clip(lower=df['salary'].quantile(0.01), upper=df['salary'].quantile(0.99))\n",
    "\n",
    "# Log transform to reduce impact (if values > 0)\n",
    "# df['salary_log'] = np.log1p(df['salary'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74515d30",
   "metadata": {},
   "source": [
    "## 5. Feature Transformation\n",
    "\n",
    "**What it does:** Applies mathematical transforms to make skewed distributions more normal or relationships more linear.\n",
    "\n",
    "**When to use:** When features are skewed or relationships to target are nonlinear.\n",
    "\n",
    "**Techniques:** Log, Box-Cox, Yeo-Johnson, Power transforms, binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Transformation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "df = pd.DataFrame({'income':[20000, 30000, 40000, 1000000]})\n",
    "\n",
    "# Log transform (handles skewness)\n",
    "# df['income_log'] = np.log1p(df['income'])\n",
    "\n",
    "# Box-Cox (requires positive data)\n",
    "# df['income_bc'], _ = boxcox(df['income'] + 1)  # add 1 if zeros present\n",
    "\n",
    "# Binning (continuous -> categorical)\n",
    "# df['income_bin'] = pd.cut(df['income'], bins=[0,30000,70000,1e7], labels=['low','mid','high'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63435a",
   "metadata": {},
   "source": [
    "## 6. Feature Creation\n",
    "\n",
    "**What it does:** Builds new features from existing ones to expose helpful signals.\n",
    "\n",
    "**When to use:** When domain knowledge suggests useful combinations or aggregations; often improves model power.\n",
    "\n",
    "**Examples:** Ratios, date-time parts, text lengths, aggregated statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622150d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Feature creation\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'loan_amount':[1000,2000,3000], 'income':[10000,20000,15000], 'date':['2020-01-01','2020-06-15','2021-03-10'], 'review':['good','bad product','excellent value']})\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Debt-to-income ratio\n",
    "# df['dti'] = df['loan_amount'] / df['income']\n",
    "\n",
    "# Extract date parts\n",
    "# df['month'] = df['date'].dt.month\n",
    "# df['weekday'] = df['date'].dt.day_name()\n",
    "\n",
    "# Text features\n",
    "# df['review_len'] = df['review'].str.len()\n",
    "# df['review_word_count'] = df['review'].str.split().apply(len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b666da",
   "metadata": {},
   "source": [
    "## 7. Feature Selection\n",
    "\n",
    "**What it does:** Chooses a subset of features that are most useful to the model.\n",
    "\n",
    "**When to use:** When dataset has many features, to reduce overfitting and improve speed.\n",
    "\n",
    "**Techniques:** Filter methods (correlation, chi2), wrapper (RFE), embedded (Lasso, tree importance), and permutation importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18ae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Feature selection\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import RFE, SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assume X, y are prepared feature matrix and target\n",
    "# X, y = ...\n",
    "\n",
    "# RFE (recursive feature elimination)\n",
    "# model = LogisticRegression(max_iter=1000)\n",
    "# rfe = RFE(model, n_features_to_select=5)\n",
    "# rfe.fit(X, y)\n",
    "# selected = rfe.support_\n",
    "\n",
    "# SelectKBest (filter)\n",
    "# skb = SelectKBest(score_func=chi2, k=10)\n",
    "# skb.fit(X, y)\n",
    "\n",
    "# Tree-based importance\n",
    "# rf = RandomForestClassifier()\n",
    "# rf.fit(X, y)\n",
    "# importances = rf.feature_importances_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b8350",
   "metadata": {},
   "source": [
    "## 8. Handling Imbalanced Data\n",
    "\n",
    "**What it does:** Balances class distribution so models don't ignore minority class.\n",
    "\n",
    "**When to use:** For classification problems with skewed class ratios.\n",
    "\n",
    "**Techniques:** Oversampling (SMOTE), undersampling, class weights, ensemble methods tailored to imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8da5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Imbalanced data handling\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Assume X, y prepared\n",
    "# print('Before:', Counter(y))\n",
    "\n",
    "# SMOTE (oversample minority)\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_res, y_res = sm.fit_resample(X, y)\n",
    "# print('After SMOTE:', Counter(y_res))\n",
    "\n",
    "# Random undersampling\n",
    "# rus = RandomUnderSampler(random_state=42)\n",
    "# X_ru, y_ru = rus.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14190a78",
   "metadata": {},
   "source": [
    "## 9. Dimensionality Reduction\n",
    "\n",
    "**What it does:** Reduces number of features while preserving variance/structure.\n",
    "\n",
    "**When to use:** When many features lead to high-dimensional data, for visualization or to reduce noise.\n",
    "\n",
    "**Techniques:** PCA, LDA, t-SNE, UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67257f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume X, y prepared\n",
    "# PCA example\n",
    "# pca = PCA(n_components=2)\n",
    "# X_pca = pca.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32d41c",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning\n",
    "\n",
    "**What it does:** Searches best hyperparameters for models for optimal performance.\n",
    "\n",
    "**When to use:** Before finalizing models — improves generalization and performance.\n",
    "\n",
    "**Techniques:** GridSearchCV, RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4efeeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# grid search example\n",
    "param_grid = {'n_estimators':[50,100], 'max_depth':[5,10,None]}\n",
    "# grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)\n",
    "# grid.fit(X, y)\n",
    "# print(grid.best_params_, grid.best_score_)\n",
    "\n",
    "# randomized search\n",
    "# param_dist = {'n_estimators':[50,100,200], 'max_depth':[3,5,10,None]}\n",
    "# rand = RandomizedSearchCV(RandomForestClassifier(), param_dist, n_iter=5, cv=3, random_state=42)\n",
    "# rand.fit(X, y)\n",
    "# print(rand.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cec713",
   "metadata": {},
   "source": [
    "## 11. Cross-Validation\n",
    "\n",
    "**What it does:** Evaluates model stability by training/testing on multiple splits.\n",
    "\n",
    "**When to use:** Always recommended rather than single train/test split, especially with limited data.\n",
    "\n",
    "**Techniques:** K-Fold, Stratified K-Fold, Time Series split, Leave-One-Out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1533af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Cross-validation\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# k-fold CV\n",
    "# scores = cross_val_score(LogisticRegression(max_iter=1000), X, y, cv=5)\n",
    "# print('CV scores:', scores, 'mean:', scores.mean())\n",
    "\n",
    "# StratifiedKFold for classification\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "# scores_skf = cross_val_score(LogisticRegression(max_iter=1000), X, y, cv=skf)\n",
    "\n",
    "# TimeSeriesSplit for time-ordered data\n",
    "# tss = TimeSeriesSplit(n_splits=5)\n",
    "# scores_ts = cross_val_score(RandomForestClassifier(), X, y, cv=tss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8384780",
   "metadata": {},
   "source": [
    "## 12. Regularization\n",
    "\n",
    "**What it does:** Adds penalty to model complexity to reduce overfitting.\n",
    "\n",
    "**When to use:** Use when model overfits; common in linear models and neural networks.\n",
    "\n",
    "**Types:** L1 (Lasso), L2 (Ridge), ElasticNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ebf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Regularization\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "\n",
    "# Lasso (L1) - can do feature selection by zeroing coeffs\n",
    "# model_l1 = Lasso(alpha=0.1)\n",
    "# model_l1.fit(X, y)\n",
    "\n",
    "# Ridge (L2) - shrink coefficients\n",
    "# model_l2 = Ridge(alpha=1.0)\n",
    "# model_l2.fit(X, y)\n",
    "\n",
    "# ElasticNet (mix of L1 & L2)\n",
    "# model_en = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "# model_en.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbca5db",
   "metadata": {},
   "source": [
    "## 13. Ensemble Learning\n",
    "\n",
    "**What it does:** Combines multiple models to get better predictions than single models.\n",
    "\n",
    "**When to use:** When single models are unstable or to boost performance.\n",
    "\n",
    "**Types:** Bagging (RandomForest), Boosting (AdaBoost, GradientBoosting, XGBoost, LightGBM, CatBoost), Stacking/Blending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29246852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Ensemble methods\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Random Forest (bagging)\n",
    "# rf = RandomForestClassifier(n_estimators=100)\n",
    "# rf.fit(X, y)\n",
    "\n",
    "# AdaBoost (boosting)\n",
    "# ada = AdaBoostClassifier(n_estimators=50)\n",
    "# ada.fit(X, y)\n",
    "\n",
    "# Gradient Boosting\n",
    "# gb = GradientBoostingClassifier()\n",
    "# gb.fit(X, y)\n",
    "\n",
    "# Stacking example\n",
    "# estimators = [('svc', SVC(probability=True)), ('rf', RandomForestClassifier())]\n",
    "# stack = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\n",
    "# stack.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf341483",
   "metadata": {},
   "source": [
    "## 14. Model Evaluation Metrics\n",
    "\n",
    "**What it does:** Quantifies how well a model performs using appropriate metrics.\n",
    "\n",
    "**When to use:** Always after training; pick metrics matching problem (classification vs regression, imbalanced data, business cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf26801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# For classification\n",
    "# print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "# print('Precision:', precision_score(y_test, y_pred))\n",
    "# print('Recall:', recall_score(y_test, y_pred))\n",
    "# print('F1:', f1_score(y_test, y_pred))\n",
    "# print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "# print('ROC AUC:', roc_auc_score(y_test, y_score_prob[:,1]))\n",
    "\n",
    "# For regression\n",
    "# print('MSE:', mean_squared_error(y_test, y_pred))\n",
    "# print('R2:', r2_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f9001",
   "metadata": {},
   "source": [
    "## 15. Train–Test Splitting Strategies\n",
    "\n",
    "**What it does:** Splits data into training and testing sets in ways suitable for the problem.\n",
    "\n",
    "**When to use:** Always — ensures evaluation on unseen data. Use stratify for classification, time-based splits for time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690f24b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLES: Train-test split strategies\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "# Simple split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Stratified split (maintains class ratios)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# StratifiedShuffleSplit for repeated stratified splits\n",
    "# sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "# for train_idx, test_idx in sss.split(X, y):\n",
    "#     X_tr, X_te = X[train_idx], X[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb312279-3124-408b-be97-f3e80b8333e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
